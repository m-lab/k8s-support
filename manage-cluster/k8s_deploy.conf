GCE_BASE_NAME="platform-cluster"
GCE_IMAGE_FAMILY="coreos-stable"
GCE_IMAGE_PROJECT="coreos-cloud"
GCE_DISK_SIZE="100"
GCE_DISK_TYPE="pd-ssd"
GCE_NETWORK="mlab-platform-network"
GCE_K8S_SUBNET="kubernetes"
GCE_EPOXY_SUBNET="epoxy"
GCE_NET_TAGS="platform-cluster" # Comma separated list
GCE_TYPE_mlab_sandbox="n1-standard-2"
GCE_TYPE_mlab_staging="n1-standard-4"
GCE_TYPE_mlab_oti="n1-standard-8"

# Monitoring variables. Note: "prometheus" is reserved for other deployments.
PROM_BASE_NAME="prometheus-${GCE_BASE_NAME}"

# TODO (kinkade): In its current form, the service account associated with
# the GCE instances need full access to a single GCS storage bucket for the
# purposes of moving around k8s TLS files, etc. Without special configuration of
# the bucket and service account, for this to work, the service account needs
# the "storage-full" scope, which is far more permissive than we ultimately
# want. Additionally, it was discovered that the cluster would not initialize
# properly with some scopes defined, but other ones missing. As of this writing
# it is unclear which scopes are needed for the "gce" k8s cloud provider plugin
# to work as intended. To get around this for testing, we are just giving the
# "cloud-platform" scope, which is pretty much full access (and includes the
# needed "storage-full" scope). The permissions should likely be dialed back as
# we learn more.
GCE_API_SCOPES="cloud-platform"

K8S_VERSION="v1.15.10"
K8S_CNI_VERSION="v0.8.5"
K8S_CRICTL_VERSION="v1.17.0"
K8S_FLANNEL_VERSION="v0.11.0"
K8S_HELM_VERSION="v3.0.2"
# The URL to install the cert-manager CRDs uses a different format for the
# version. If you upgrade cert-manager, you will need to change both variables.
K8S_CERTMANAGER_VERSION="v0.11.0"
K8S_CERTMANAGER_RESOURCES_VERSION="release-0.11"
K8S_CA_FILES="ca.crt ca.key sa.key sa.pub front-proxy-ca.crt front-proxy-ca.key etcd/ca.crt etcd/ca.key"
K8S_PKI_DIR="/tmp/kubernetes-pki"
K8S_CLUSTER_CIDR="192.168.0.0/16"
K8S_SERVICE_CIDR="172.25.0.0/16"

K8S_CLOUD_NODE_BASE_NAME="node-platform-cluster"
K8S_CLOUD_NODE_LABELS="mlab/type=cloud"

TOKEN_SERVER_BASE_NAME="token-server"
TOKEN_SERVER_PORT="8800"

# Depending on the GCP project we may use different regions, zones, GSC buckets.
#
# Sandbox
GCE_REGION_mlab_sandbox="us-west2"
GCE_ZONES_mlab_sandbox="a b c"
GCS_BUCKET_EPOXY_mlab_sandbox="epoxy-mlab-sandbox"
GCS_BUCKET_K8S_mlab_sandbox="k8s-support-mlab-sandbox"
GCS_BUCKET_SITEINFO_mlab_sandbox="siteinfo-mlab-sandbox"

# Staging
GCE_REGION_mlab_staging="us-central1"
GCE_ZONES_mlab_staging="a b c"
GCS_BUCKET_EPOXY_mlab_staging="epoxy-mlab-staging"
GCS_BUCKET_K8S_mlab_staging="k8s-support-mlab-staging"
GCS_BUCKET_SITEINFO_mlab_staging="siteinfo-mlab-staging"

# Production
GCE_REGION_mlab_oti="us-east1"
GCE_ZONES_mlab_oti="b c d"
GCS_BUCKET_EPOXY_mlab_oti="epoxy-mlab-oti"
GCS_BUCKET_K8S_mlab_oti="k8s-support-mlab-oti"
GCS_BUCKET_SITEINFO_mlab_oti="siteinfo-mlab-oti"

# The days on which the master nodes will be rebooted automatically. The days
# map to three GCE_ZONES defined for each project. That is, the first day in
# the below array will apply to the first GCE_ZONE defined for the project, and
# so on.
REBOOT_DAYS=(Tue Wed Thu)

# Configurations for setting the --max-rate flag in ndt-server.  For 1g sites
# we are starting with a conservative value of 40% of the uplink, as this will
# be applied per-node. For 10g sites we use a more liberal value of 80%. The
# values are in bits.
MAX_RATES_DIR="nodes-max-rate"
MAX_RATES_CONFIGMAP="nodes-max-rate"
MAX_RATE_1G="400000000"
MAX_RATE_10G="8000000000"

# Whether the script should exit after deleting all existing GCP resources
# associated with creating this k8s cluster. This could be useful, for example,
# if you want to change various object names, but don't want to have to
# manually hunt down all the old objects all over the GCP console. For
# example, many objects names are based on the variable $GCE_BASE_NAME. If you
# were to assign another value to that variable and run this script, any old,
# existing objects will not be removed, and will linger orphaned in the GCP
# project. One way to use this would be to set the following to "yes", run this
# script, _then_ change any base object names, reset this to "no" and run this
# script.
EXIT_AFTER_DELETE="no"
